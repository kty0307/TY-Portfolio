{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주식정보수집 for Human Deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 기본설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import fitz\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. S&P500 목록 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>YUM</td>\n",
       "      <td>Yum! Brands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>ZBRA</td>\n",
       "      <td>Zebra Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ZBH</td>\n",
       "      <td>Zimmer Biomet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>ZION</td>\n",
       "      <td>Zions Bancorporation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>Zoetis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker               Company\n",
       "0      MMM                    3M\n",
       "1      AOS           A. O. Smith\n",
       "2      ABT                Abbott\n",
       "3     ABBV                AbbVie\n",
       "4      ACN             Accenture\n",
       "..     ...                   ...\n",
       "498    YUM           Yum! Brands\n",
       "499   ZBRA    Zebra Technologies\n",
       "500    ZBH         Zimmer Biomet\n",
       "501   ZION  Zions Bancorporation\n",
       "502    ZTS                Zoetis\n",
       "\n",
       "[503 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S&P500 기업 정보 데이터 불러오기\n",
    "sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "\n",
    "# 필요한 열 선택하기\n",
    "sp500 = sp500[['Symbol', 'Security']]\n",
    "\n",
    "# 열 이름 변경하기\n",
    "sp500 = sp500.rename(columns={'Symbol': 'Ticker', 'Security': 'Company'})\n",
    "\n",
    "# 데이터프레임 출력하기\n",
    "sp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBatch(df, batch_size=50):\n",
    "    bucket = []\n",
    "    num_batch = len(df) // batch_size\n",
    "    for n_th in range(num_batch+1):\n",
    "        if (n_th+1)*batch_size < len(df):\n",
    "            df_batch = df.iloc[n_th*batch_size: (n_th+1)*batch_size]\n",
    "        else:\n",
    "            df_batch = df.iloc[n_th*batch_size: ]\n",
    "        bucket.append(df_batch)\n",
    "    return bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_batches = makeBatch(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. 날짜정리(for Sentiment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan = pd.read_csv('./SP500-dataset-withRs/AAL-withRs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return</th>\n",
       "      <th>calculated_price</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-10-07</td>\n",
       "      <td>20.900</td>\n",
       "      <td>23.00</td>\n",
       "      <td>20.90</td>\n",
       "      <td>22.15</td>\n",
       "      <td>16350134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.15</td>\n",
       "      <td>[0.0, 0.005417607223476395, -0.013920071845532...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-10-14</td>\n",
       "      <td>22.280</td>\n",
       "      <td>22.40</td>\n",
       "      <td>21.40</td>\n",
       "      <td>22.27</td>\n",
       "      <td>9746113</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>22.27</td>\n",
       "      <td>[0.005417607223476395, -0.013920071845532012, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-10-21</td>\n",
       "      <td>22.150</td>\n",
       "      <td>22.42</td>\n",
       "      <td>20.85</td>\n",
       "      <td>21.96</td>\n",
       "      <td>14218231</td>\n",
       "      <td>-0.013920</td>\n",
       "      <td>21.96</td>\n",
       "      <td>[-0.013920071845532012, 0.08834244080145703, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-10-28</td>\n",
       "      <td>22.010</td>\n",
       "      <td>23.90</td>\n",
       "      <td>21.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>9263807</td>\n",
       "      <td>0.088342</td>\n",
       "      <td>23.90</td>\n",
       "      <td>[0.08834244080145703, 0.20502092050209209, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-11-04</td>\n",
       "      <td>24.000</td>\n",
       "      <td>29.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>28.80</td>\n",
       "      <td>16788473</td>\n",
       "      <td>0.205021</td>\n",
       "      <td>28.80</td>\n",
       "      <td>[0.20502092050209209, 0.15798611111111116, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>13.960</td>\n",
       "      <td>14.76</td>\n",
       "      <td>13.71</td>\n",
       "      <td>14.75</td>\n",
       "      <td>70608911</td>\n",
       "      <td>0.075073</td>\n",
       "      <td>14.75</td>\n",
       "      <td>[0.07507288629737596, -0.050169491525423715, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>14.520</td>\n",
       "      <td>14.62</td>\n",
       "      <td>13.70</td>\n",
       "      <td>14.01</td>\n",
       "      <td>63721777</td>\n",
       "      <td>-0.050169</td>\n",
       "      <td>14.01</td>\n",
       "      <td>[-0.050169491525423715, -0.08708065667380449, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>2023-04-14</td>\n",
       "      <td>13.980</td>\n",
       "      <td>14.51</td>\n",
       "      <td>12.73</td>\n",
       "      <td>12.79</td>\n",
       "      <td>208236734</td>\n",
       "      <td>-0.087081</td>\n",
       "      <td>12.79</td>\n",
       "      <td>[-0.08708065667380449, 0.04769351055512128, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>2023-04-21</td>\n",
       "      <td>12.855</td>\n",
       "      <td>13.70</td>\n",
       "      <td>12.80</td>\n",
       "      <td>13.40</td>\n",
       "      <td>123132547</td>\n",
       "      <td>0.047694</td>\n",
       "      <td>13.40</td>\n",
       "      <td>[0.04769351055512128, -0.04925373134328359]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>2023-04-26</td>\n",
       "      <td>13.420</td>\n",
       "      <td>13.56</td>\n",
       "      <td>12.65</td>\n",
       "      <td>12.74</td>\n",
       "      <td>91770378</td>\n",
       "      <td>-0.049254</td>\n",
       "      <td>12.74</td>\n",
       "      <td>[-0.04925373134328359]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>917 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp    open   high    low  close     volume    return  \\\n",
       "0    2005-10-07  20.900  23.00  20.90  22.15   16350134  0.000000   \n",
       "1    2005-10-14  22.280  22.40  21.40  22.27    9746113  0.005418   \n",
       "2    2005-10-21  22.150  22.42  20.85  21.96   14218231 -0.013920   \n",
       "3    2005-10-28  22.010  23.90  21.70  23.90    9263807  0.088342   \n",
       "4    2005-11-04  24.000  29.00  24.00  28.80   16788473  0.205021   \n",
       "..          ...     ...    ...    ...    ...        ...       ...   \n",
       "912  2023-03-31  13.960  14.76  13.71  14.75   70608911  0.075073   \n",
       "913  2023-04-06  14.520  14.62  13.70  14.01   63721777 -0.050169   \n",
       "914  2023-04-14  13.980  14.51  12.73  12.79  208236734 -0.087081   \n",
       "915  2023-04-21  12.855  13.70  12.80  13.40  123132547  0.047694   \n",
       "916  2023-04-26  13.420  13.56  12.65  12.74   91770378 -0.049254   \n",
       "\n",
       "     calculated_price                                                  R  \n",
       "0               22.15  [0.0, 0.005417607223476395, -0.013920071845532...  \n",
       "1               22.27  [0.005417607223476395, -0.013920071845532012, ...  \n",
       "2               21.96  [-0.013920071845532012, 0.08834244080145703, 0...  \n",
       "3               23.90  [0.08834244080145703, 0.20502092050209209, 0.1...  \n",
       "4               28.80  [0.20502092050209209, 0.15798611111111116, -0....  \n",
       "..                ...                                                ...  \n",
       "912             14.75  [0.07507288629737596, -0.050169491525423715, -...  \n",
       "913             14.01  [-0.050169491525423715, -0.08708065667380449, ...  \n",
       "914             12.79  [-0.08708065667380449, 0.04769351055512128, -0...  \n",
       "915             13.40        [0.04769351055512128, -0.04925373134328359]  \n",
       "916             12.74                             [-0.04925373134328359]  \n",
       "\n",
       "[917 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan['timestamp'] = pd.to_datetime(timespan['timestamp'])\n",
    "timespan = timespan[timespan['timestamp'] >= datetime.datetime(2018, 1, 1)]\n",
    "timespan['timestamp'] = timespan['timestamp'].dt.strftime('%Y%m%d')\n",
    "timespan_list = timespan['timestamp'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timespan_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. download함수 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import time \n",
    "\n",
    "## 제공파일타입이 CSV인 경우\n",
    "def downloadCSV(dfTickers):\n",
    "    list_tickers = dfTickers['Ticker'].tolist()\n",
    "    print(f'corps: {list_tickers}')\n",
    "    \n",
    "    for t in list_tickers:\n",
    "        u = f'https://www.alphavantage.co/query?function={function}&symbol={t}&datatype=csv&apikey={ALPHA_VINTAGE_API_KEY}'\n",
    "        r = requests.get(u)\n",
    "        webbrowser.open_new_tab(r.url)\n",
    "        time.sleep(25) # 20초로 해도 5콜/분 한도 초과라 여유있게 25초로 설정\n",
    "\n",
    "## 제공파일타입이 JSON인 경우\n",
    "def downloadJSON(dfTickers, function):\n",
    "    list_tickers = dfTickers['Ticker'].tolist()\n",
    "    print(f'corps: {list_tickers}')\n",
    "    \n",
    "    if function == 'NEWS_SENTIMENT':\n",
    "        for t in list_tickers:\n",
    "            path = f'./sp500-dataset-all/{function}/{t}.json'\n",
    "            u = f'https://www.alphavantage.co/query?function={function}&time_from={time_from}&time_to={time_to}&sort={sort}&limit={limit}&tickers={t}&apikey={api_key}'\n",
    "            r = requests.get(u)\n",
    "            r_json = r.json()\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(r_json, f)\n",
    "            time.sleep(25) # 20초로 해도 5콜/분 한도 초과라 여유있게 25초로 설정\n",
    "    else :\n",
    "        for t in list_tickers:\n",
    "            path = f'./sp500-dataset-all/{function}/{t}.json'\n",
    "            u = f'https://www.alphavantage.co/query?function={function}&symbol={t}&apikey={api_key}'\n",
    "            r = requests.get(u)\n",
    "            r_json = r.json()\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(r_json, f)\n",
    "            time.sleep(25) # 20초로 해도 5콜/분 한도 초과라 여유있게 25초로 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. API key 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'API'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. JSON to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 부문별 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재무정보\n",
    "function_income = 'INCOME_STATEMENT'\n",
    "\n",
    "# 재무제표\n",
    "function_balance = 'BALANCE_SHEET'\n",
    "\n",
    "# 현금흐름\n",
    "function_cashflow = 'CASH_FLOW'\n",
    "\n",
    "# 수익\n",
    "function_earnings = 'EARNINGS'\n",
    "\n",
    "# Some Metrics\n",
    "function_metrics = 'OVERVIEW'\n",
    "\n",
    "# News Sentiment\n",
    "function_sentiment = 'NEWS_SENTIMENT'\n",
    "time_from = [s + 'T0000' for s in timespan_list]\n",
    "time_to = [s + 'T1600' for s in timespan_list]\n",
    "\n",
    "\n",
    "'20220410T0130' # YYYYMMDDTHHMM 형식\n",
    "time_to = ''\n",
    "sort = 'EARLIEST'\n",
    "limit = '50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 준영님\n",
    "downloadJSON(list_batches[0], function_income)\n",
    "downloadJSON(list_batches[1], function_income)\n",
    "downloadJSON(list_batches[2], function_income)\n",
    "downloadJSON(list_batches[3], function_income)\n",
    "downloadJSON(list_batches[4], function_income)\n",
    "downloadJSON(list_batches[5], function_income)\n",
    "downloadJSON(list_batches[6], function_income)\n",
    "downloadJSON(list_batches[7], function_income)\n",
    "downloadJSON(list_batches[8], function_income)\n",
    "downloadJSON(list_batches[9], function_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해솔님\n",
    "downloadJSON(list_batches[0], function_balance)\n",
    "downloadJSON(list_batches[1], function_balance)\n",
    "downloadJSON(list_batches[2], function_balance)\n",
    "downloadJSON(list_batches[3], function_balance)\n",
    "downloadJSON(list_batches[4], function_balance)\n",
    "downloadJSON(list_batches[5], function_balance)\n",
    "downloadJSON(list_batches[6], function_balance)\n",
    "downloadJSON(list_batches[7], function_balance)\n",
    "downloadJSON(list_batches[8], function_balance)\n",
    "downloadJSON(list_batches[9], function_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채환님\n",
    "downloadJSON(list_batches[0], function_cashflow)\n",
    "downloadJSON(list_batches[1], function_cashflow)\n",
    "downloadJSON(list_batches[2], function_cashflow)\n",
    "downloadJSON(list_batches[3], function_cashflow)\n",
    "downloadJSON(list_batches[4], function_cashflow)\n",
    "downloadJSON(list_batches[5], function_cashflow)\n",
    "downloadJSON(list_batches[6], function_cashflow)\n",
    "downloadJSON(list_batches[7], function_cashflow)\n",
    "downloadJSON(list_batches[8], function_cashflow)\n",
    "downloadJSON(list_batches[9], function_cashflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성희님\n",
    "downloadJSON(list_batches[0], function_earnings)\n",
    "downloadJSON(list_batches[1], function_earnings)\n",
    "downloadJSON(list_batches[2], function_earnings)\n",
    "downloadJSON(list_batches[3], function_earnings)\n",
    "downloadJSON(list_batches[4], function_earnings)\n",
    "downloadJSON(list_batches[5], function_earnings)\n",
    "downloadJSON(list_batches[6], function_earnings)\n",
    "downloadJSON(list_batches[7], function_earnings)\n",
    "downloadJSON(list_batches[8], function_earnings)\n",
    "downloadJSON(list_batches[9], function_earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corps: ['ZBH', 'ZION', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "# 누락분\n",
    "# downloadJSON(list_batches[10], function_cashflow)\n",
    "# downloadJSON(list_batches[10], function_earnings)\n",
    "# downloadJSON(list_batches[10], function_income)\n",
    "downloadJSON(list_batches[10], function_balance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error 수작업\n",
    "\n",
    "# 목록 \n",
    "# CASH_FLOW : ADM, ANET, APD, PAYX, PHM, SBUX, VICI / BRK.B, BF.B\n",
    "# EARNINGS : / BRK.B, BF.B\n",
    "# BALANCE_SHEET : / BRK.B, BF.B\n",
    "# INCOME_STATEMENT : CTAS, CTRA, DGX, DRI, KO, MCO, PEG, RF, SPGI, GEHC, CEG, OGN, FRC, VTRS, CARR, OTIS, FOX, FOXA / BRK.B, BF.B\n",
    "\n",
    "\n",
    "error_function = 'INCOME_STATEMENT'\n",
    "error_t = 'FOXA'\n",
    "\n",
    "error_path = f'./sp500-dataset-all/{error_function}/{error_t}.json'\n",
    "error_u = f'https://www.alphavantage.co/query?function={error_function}&symbol={error_t}&apikey={api_key}'\n",
    "error_r = requests.get(error_u)\n",
    "error_r_json = error_r.json()\n",
    "with open(error_path, \"w\") as f:\n",
    "    json.dump(error_r_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INCOME_STATEMENT\n",
    "\n",
    "# 폴더 경로 지정\n",
    "folder_path = './SP500-dataset-all/INCOME_STATEMENT'\n",
    "# 폴더 내 모든 파일의 경로 가져오기\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)]\n",
    "# 파일 목록 출력\n",
    "file_paths\n",
    "\n",
    "for i in file_paths : \n",
    "    try:\n",
    "        with open(i, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f'Error: Failed to decode JSON in {i}. {e}')\n",
    "        continue\n",
    "    filename, ext = os.path.splitext(i)\n",
    "    filename_only = os.path.basename(filename)\n",
    "    temp = pd.json_normalize(data['quarterlyReports'])\n",
    "    temp.to_csv(f'{folder_path}_csv/{filename_only}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BALANCE_SHEET\n",
    "\n",
    "# 폴더 경로 지정\n",
    "folder_path = './SP500-dataset-all/BALANCE_SHEET'\n",
    "# 폴더 내 모든 파일의 경로 가져오기\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)]\n",
    "# 파일 목록 출력\n",
    "file_paths\n",
    "\n",
    "for i in file_paths : \n",
    "    try:\n",
    "        with open(i, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f'Error: Failed to decode JSON in {i}. {e}')\n",
    "        continue\n",
    "    filename, ext = os.path.splitext(i)\n",
    "    filename_only = os.path.basename(filename)\n",
    "    temp = pd.json_normalize(data['quarterlyReports'])\n",
    "    temp.to_csv(f'{folder_path}_csv/{filename_only}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CASH_FLOW\n",
    "\n",
    "# 폴더 경로 지정\n",
    "folder_path = './SP500-dataset-all/CASH_FLOW'\n",
    "# 폴더 내 모든 파일의 경로 가져오기\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)]\n",
    "# 파일 목록 출력\n",
    "file_paths\n",
    "\n",
    "for i in file_paths : \n",
    "    try:\n",
    "        with open(i, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f'Error: Failed to decode JSON in {i}. {e}')\n",
    "        continue\n",
    "    filename, ext = os.path.splitext(i)\n",
    "    filename_only = os.path.basename(filename)\n",
    "    temp = pd.json_normalize(data['quarterlyReports'])\n",
    "    temp.to_csv(f'{folder_path}_csv/{filename_only}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EARNINGS\n",
    "\n",
    "# 폴더 경로 지정\n",
    "folder_path = './SP500-dataset-all/EARNINGS'\n",
    "# 폴더 내 모든 파일의 경로 가져오기\n",
    "file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)]\n",
    "# 파일 목록 출력\n",
    "file_paths\n",
    "\n",
    "for i in file_paths : \n",
    "    try:\n",
    "        with open(i, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f'Error: Failed to decode JSON in {i}. {e}')\n",
    "        continue\n",
    "    filename, ext = os.path.splitext(i)\n",
    "    filename_only = os.path.basename(filename)\n",
    "    temp = pd.json_normalize(data['quarterlyEarnings'])\n",
    "    temp.to_csv(f'{folder_path}_csv/{filename_only}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OVERVIEW\n",
    "\n",
    "# # 폴더 경로 지정\n",
    "# folder_path = './SP500-dataset-all/OVERVIEW'\n",
    "# # 폴더 내 모든 파일의 경로 가져오기\n",
    "# file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path)]\n",
    "# # 파일 목록 출력\n",
    "# file_paths\n",
    "\n",
    "# for i in file_paths : \n",
    "#     with open(i, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     filename, ext = os.path.splitext(i)\n",
    "#     filename_only = os.path.basename(filename)\n",
    "#     temp = pd.json_normalize(data['quarterlyReports'])\n",
    "#     temp.to_csv(f'{folder_path}_csv/{filename_only}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generaterow(i, folder_path, keydate): # folder_path는 folder_path_INCOME_STATEMENT와 같이, keydate은 fiscalDateEnding인지 reportedDate(EARNINGS만)인지\n",
    "    filename, ext = os.path.splitext(i)\n",
    "    filename_only = os.path.basename(filename)\n",
    "    df = pd.read_csv(f'{folder_path}/{filename_only}.csv')\n",
    "    df.set_index(pd.to_datetime(df[keydate]), inplace=True)\n",
    "    df.drop(columns=[keydate], inplace=True)\n",
    "    df_resampled = df.resample('D').asfreq()\n",
    "    df_resampled.fillna(method='ffill', inplace=True)\n",
    "    last_date = df_resampled.index[-1]\n",
    "    today = datetime.date.today()\n",
    "    date_range = pd.date_range(last_date, today, freq='D')[1:]\n",
    "    df_resampled = pd.concat([df_resampled, pd.DataFrame(index=date_range)], axis=0)\n",
    "    df_resampled.fillna(method='ffill', inplace=True)\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    df_resampled.rename(columns={'index': keydate}, inplace=True)\n",
    "    df_resampled.to_csv(f'{folder_path}_process/{filename_only}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Merge를 위한 row 증폭 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 경로 지정\n",
    "folder_path_INCOME_STATEMENT = './SP500-dataset-all/INCOME_STATEMENT_csv'\n",
    "folder_path_BALANCE_SHEET = './SP500-dataset-all/BALANCE_SHEET_csv'\n",
    "folder_path_CASH_FLOW = './SP500-dataset-all/CASH_FLOW_csv'\n",
    "folder_path_EARNINGS = './SP500-dataset-all/EARNINGS_csv'\n",
    "\n",
    "# 폴더 내 모든 파일의 경로 가져오기\n",
    "file_paths_INCOME_STATEMENT = [os.path.join(folder_path_INCOME_STATEMENT, file) for file in os.listdir(folder_path_INCOME_STATEMENT)]\n",
    "file_paths_BALANCE_SHEET = [os.path.join(folder_path_BALANCE_SHEET, file) for file in os.listdir(folder_path_BALANCE_SHEET)]\n",
    "file_paths_CASH_FLOW = [os.path.join(folder_path_CASH_FLOW, file) for file in os.listdir(folder_path_CASH_FLOW)]\n",
    "file_paths_EARNINGS = [os.path.join(folder_path_EARNINGS, file) for file in os.listdir(folder_path_EARNINGS)]\n",
    "\n",
    "# merge를 위한 전처리\n",
    "for i in file_paths_INCOME_STATEMENT : \n",
    "    generaterow(i, folder_path_INCOME_STATEMENT, 'fiscalDateEnding')\n",
    "\n",
    "# for i in file_paths_BALANCE_SHEET :\n",
    "#     generaterow(i, folder_path_BALANCE_SHEET, 'fiscalDateEnding')\n",
    "\n",
    "# for i in file_paths_CASH_FLOW :\n",
    "#     generaterow(i, folder_path_CASH_FLOW, 'fiscalDateEnding')\n",
    "\n",
    "# for i in file_paths_EARNINGS :\n",
    "#     generaterow(i, folder_path_EARNINGS, 'reportedDate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARNINGS GPN 추가 처리\n",
    "\n",
    "df_i = pd.read_csv(f'./SP500-dataset-all/GPN.csv')\n",
    "df_i.set_index(pd.to_datetime(df_i['reportedDate']), inplace=True)\n",
    "df_i.drop_duplicates(subset=['reportedDate'], inplace=True)\n",
    "df_i_resampled = df_i.resample('D').asfreq()\n",
    "df_i_resampled.fillna(method='ffill', inplace=True)\n",
    "last_date = df_i_resampled.index[-1]\n",
    "today = datetime.date.today()\n",
    "date_range = pd.date_range(last_date, today, freq='D')[1:]\n",
    "df_i_resampled = pd.concat([df_i_resampled, pd.DataFrame(index=date_range)], axis=0)\n",
    "df_i_resampled.fillna(method='ffill', inplace=True)\n",
    "df_i_resampled.reset_index(inplace=True)\n",
    "df_i_resampled.rename(columns={'index':'reportedDate'}, inplace=True)\n",
    "\n",
    "df_i_resampled.to_csv('./SP500-dataset-all/GPN_process.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1. INCOME_STATEMENT, BALANCE_SHEET, CASH_FLOW, EARNINGS 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = set()\n",
    "for file_list in [file_paths_INCOME_STATEMENT, file_paths_BALANCE_SHEET, file_paths_CASH_FLOW, folder_path_EARNINGS]:\n",
    "    file_names.update([os.path.splitext(os.path.basename(f))[0] for f in file_list])\n",
    "\n",
    "for file_name in file_names:\n",
    "    income_file = f'./SP500-dataset-all/INCOME_STATEMENT_csv_process/{file_name}.csv'\n",
    "    balance_file = f'./SP500-dataset-all/BALANCE_SHEET_csv_process/{file_name}.csv'\n",
    "    cash_flow_file = f'./SP500-dataset-all/CASH_FLOW_csv_process/{file_name}.csv'\n",
    "    earnings_file = f'./SP500-dataset-all/EARNINGS_csv_process/{file_name}.csv'\n",
    "\n",
    "    if os.path.exists(income_file) and os.path.exists(balance_file) and os.path.exists(cash_flow_file) and os.path.exists(earnings_file):\n",
    "        df1 = pd.read_csv(income_file)\n",
    "        df2 = pd.read_csv(balance_file)\n",
    "        df3 = pd.read_csv(cash_flow_file)\n",
    "        df4 = pd.read_csv(earnings_file)\n",
    "        merged_df = pd.merge(df1, df2, on='fiscalDateEnding')\n",
    "        merged_df = pd.merge(merged_df, df3, on='fiscalDateEnding')\n",
    "        merged_df = pd.concat([merged_df.set_index('fiscalDateEnding'), df4.set_index('reportedDate')], axis=1, join='outer')\n",
    "        merged_df.to_csv(f'./SP500-dataset-all/MERGED/{file_name}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. primary key name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './SP500-dataset-all/MERGED/'\n",
    "\n",
    "file_names = set()\n",
    "for f in os.listdir(directory_path):\n",
    "    if os.path.isfile(os.path.join(directory_path, f)):\n",
    "        file_names.update([os.path.splitext(os.path.basename(f))[0]])\n",
    "\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary key인 첫번째열 timestamp로 이름 정의, fiscalDateEnding열 삭제\n",
    "for file_name in file_names:\n",
    "    df_rename = pd.read_csv(f'{directory_path}{file_name}.csv')\n",
    "    first_column_name = df_rename.columns[0]\n",
    "    df_rename = df_rename.rename(columns={first_column_name: 'timestamp'})\n",
    "    df_rename = df_rename.drop(columns='fiscalDateEnding')\n",
    "    df_rename.to_csv(f'./SP500-dataset-all/MERGED_rename/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_allmost = pd.read_csv('./SP500-dataset-all/MERGED_rename/AAL.csv')\n",
    "type_diff = pd.read_csv('./SP500-dataset-all/MERGED_rename/GPN.csv')\n",
    "\n",
    "a = list(type_allmost.columns)\n",
    "b = list(type_diff.columns)\n",
    "\n",
    "different_elements = set(a).symmetric_difference(set(b))\n",
    "\n",
    "# 다른 요소를 출력합니다.\n",
    "print(\"Different elements between list1 and list2:\")\n",
    "for element in different_elements:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./SP500-dataset-all/MERGED_rename/GPN.csv')\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "df = df.drop(columns='reportedDate.1')\n",
    "df.to_csv('./SP500-dataset-all/MERGED_rename/GPN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3. 주식가격 정보와 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './SP500-dataset-withRs/'\n",
    "\n",
    "file_names = set()\n",
    "for f in os.listdir(directory_path):\n",
    "    if os.path.isfile(os.path.join(directory_path, f)):\n",
    "        file_name, _ = os.path.splitext(os.path.basename(f))\n",
    "        file_name = file_name.replace('-withRs', '')  # '-withRs' 문자열 제거\n",
    "        file_names.update([file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    price_file = f'./SP500-dataset-withRs/{file_name}-withRs.csv'\n",
    "    notprice_file = f'./SP500-dataset-all/MERGED_rename/{file_name}.csv'\n",
    "\n",
    "    if os.path.exists(price_file) and os.path.exists(notprice_file):\n",
    "        try:\n",
    "            df1 = pd.read_csv(price_file, error_bad_lines=False)\n",
    "            df2 = pd.read_csv(notprice_file)\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"ParserError in file: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in file: {file_name}, Error: {e}\")\n",
    "        else:\n",
    "            df1 = df1.drop(columns='R')\n",
    "            merged_df = pd.merge(df1, df2, on='timestamp', how='inner')\n",
    "            merged_df.to_csv(f'./SP500-dataset-all/MERGED_price/{file_name}.csv', index=False)\n",
    "\n",
    "# 해당 처리로 AMAT, TEL, EOG, CNC, VTR은 제외되었음(해당 기업은 모두 2018년 이전 데이터가 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.4. 개별기업 파일 전처리(2018년 이후 데이터만 취급)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './SP500-dataset-all/MERGED_price/'\n",
    "start_date = datetime.strptime('2018-01-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2022-04-30', '%Y-%m-%d')\n",
    "\n",
    "for f in os.listdir(directory_path):\n",
    "    file_path = os.path.join(directory_path, f)\n",
    "    file_name, _ = os.path.splitext(os.path.basename(f))\n",
    "\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            \n",
    "            min_date = df['timestamp'].min()\n",
    "            max_date = df['timestamp'].max()\n",
    "\n",
    "            if min_date > start_date and max_date < end_date:\n",
    "                print(f\"Incomplete date range in file: {f}\")\n",
    "            else:\n",
    "                df = df[df['timestamp'] >= start_date]\n",
    "                df.to_csv(f'./SP500-dataset-all/FINAL/{file_name}.csv', index=False)\n",
    "        else:\n",
    "            print(f\"'timestamp' column not found in file: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './SP500-dataset-all/FINAL/'\n",
    "csv_files = [f for f in os.listdir(directory_path)]\n",
    "error_file = []\n",
    "error_file_name = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    name = os.path.splitext(csv_file)[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    row_count = len(df)\n",
    "    if row_count < 278:\n",
    "        error_file.append(f'{csv_file}_{row_count}rows')\n",
    "        error_file_name.append(f'{name}')\n",
    "    print(f\"Number of rows in {csv_file}: {row_count}\")\n",
    "\n",
    "\n",
    "error_file # ETN, PEG는 삭제하고 278개의 행이 모두 있는 버전과 불완전 행도 모두 포함시킨 버전 두개를 만들것\n",
    "error_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 통합파일 생성\n",
    "- 티커열 생성(2.3.1.)\n",
    "- 278개 행이 모두 있는 버전, 불완전 행 기업도 포함한 버전 두 개의 파일을 생성(2.3.1.)\n",
    "- 13주 붙임(2.3.2.)\n",
    "- 값없는 열이 포함된 행 제거(2.3.3.)\n",
    "- train set 분리(2.3.4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. 티커열생성 및 통합파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './SP500-dataset-all/FINAL/'\n",
    "csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store the final merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove the file extension and create a ticker column\n",
    "    ticker = os.path.splitext(csv_file)[0]\n",
    "    df.insert(0, 'ticker', ticker)\n",
    "    \n",
    "    # Append the current DataFrame to the merged_data DataFrame\n",
    "    merged_data = merged_data.append(df, ignore_index=True)\n",
    "\n",
    "# Save the merged_data DataFrame as a CSV file\n",
    "merged_data.to_csv('./SP500-dataset-all/merged_data_witherror.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the final merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Remove the file extension and create a ticker column\n",
    "    ticker = os.path.splitext(csv_file)[0]\n",
    "    df.insert(0, 'ticker', ticker)\n",
    "    \n",
    "    # Append the current DataFrame to the merged_data DataFrame\n",
    "    if ticker in error_file_name:\n",
    "        pass\n",
    "    else:\n",
    "        merged_data = merged_data.append(df, ignore_index=True)\n",
    "\n",
    "# Save the merged_data DataFrame as a CSV file\n",
    "merged_data.to_csv('./SP500-dataset-all/merged_data_withouterror.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. P, P_future, R, R_future 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# witherror파일\n",
    "df1 = pd.read_csv('./SP500-dataset-all/merged_data_witherror.csv')\n",
    "df2 = pd.read_csv('dataset_GPT_model.csv')\n",
    "\n",
    "# 두 테이블 합치기\n",
    "merged_df = pd.merge(df1, df2[['ticker', 'timestamp', 'P', 'P_future', 'R', 'R_future']], on=['ticker', 'timestamp'])\n",
    "merged_df.to_csv('./SP500-dataset-all/merged_data_witherror_labeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withouterror파일\n",
    "df1 = pd.read_csv('./SP500-dataset-all/merged_data_withouterror.csv')\n",
    "df2 = pd.read_csv('dataset_GPT_model.csv')\n",
    "\n",
    "# 두 테이블 합치기\n",
    "merged_df = pd.merge(df1, df2[['ticker', 'timestamp', 'P', 'P_future', 'R', 'R_future']], on=['ticker', 'timestamp'])\n",
    "merged_df.to_csv('./SP500-dataset-all/merged_data_withouterror_labeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. 빈칸(NA)이 10개 이상인 행 제거, currency열 제거, 'None'을 0으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# witherror파일\n",
    "df = pd.read_csv('./SP500-dataset-all/merged_data_witherror_labeling.csv')\n",
    "df = df.dropna(axis=0, thresh=df.shape[1] - 10)\n",
    "df = df.drop(columns=['reportedCurrency_x', 'reportedCurrency_y', 'reportedCurrency'])\n",
    "df = df.fillna(value=0)\n",
    "df = df.replace('None', 0)\n",
    "df.to_csv('./SP500-dataset-all/dataset_human_model_witherror.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withouterror파일\n",
    "df = pd.read_csv('./SP500-dataset-all/merged_data_withouterror_labeling.csv')\n",
    "df = df.dropna(axis=0, thresh=df.shape[1] - 10)\n",
    "df = df.drop(columns=['reportedCurrency_x', 'reportedCurrency_y', 'reportedCurrency'])\n",
    "df = df.fillna(value=0)\n",
    "df = df.replace('None', 0)\n",
    "df.to_csv('./SP500-dataset-all/dataset_human_model_withouterror.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4. train set 분리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./SP500-dataset-all/dataset_human_model_witherror.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "train_set = df[df['timestamp'] < datetime(2021, 8, 31)]\n",
    "train_set.to_csv('./SP500-dataset-all/dataset_train_human_model_witherror.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./SP500-dataset-all/dataset_human_model_withouterror.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "train_set = df[df['timestamp'] < datetime(2021, 8, 31)]\n",
    "train_set.to_csv('./SP500-dataset-all/dataset_train_human_model_withouterror.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
